import random
import faiss
import torch.distributed as dist
from torchvision.transforms import GaussianBlur
import torch
import numpy as np
import math
import torch
import warnings
from typing import Tuple
from torch import Tensor
import numpy as np
from einops import rearrange
from skimage.measure import label
import torch.nn.functional as F

@torch.no_grad()
def concat_all_gather(tensor):
    """
    Performs all_gather operation on the provided tensors.
    *** Warning ***: torch.distributed.all_gather has no gradient.
    """
    tensors_gather = [
        torch.ones_like(tensor) for _ in range(torch.distributed.get_world_size())
    ]
    torch.distributed.all_gather(tensors_gather, tensor, async_op=False)

    output = torch.cat(tensors_gather, dim=0)
    return output


@torch.no_grad()
def concat_all_gather_for_unequal_length(data):
    """
    Adopted from https://stackoverflow.com/questions/71433507/pytorch-python-distributed-multiprocessing-gather-concatenate-tensor-arrays-of
    https://github.com/facebookresearch/maskrcnn-benchmark/blob/main/maskrcnn_benchmark/utils/comm.py
    Image will have diffferent numbers of foreground pixels, so when we try to gather all the foreground pixel representations from
    multiple GPUs, the number of these representations in each gpu will be different. This functions allow gather array with
    different length.
    Run all_gather on arbitrary picklable data (not necessarily tensors)
    Args:
        data: any picklable object
    Returns:
        list[data]: list of data gathered from each rank
    """
    local_size = torch.tensor(data.size(), device="cuda")  # tensor([N,   dim], device='cuda:0')
    dim = local_size[1].item()
    ws = torch.distributed.get_world_size()
    all_sizes = [torch.zeros_like(local_size) for _ in range(ws)]
    torch.distributed.all_gather(all_sizes, local_size)
    # when we have two gpus, all_sizes = [tensor([N1,   dim], device='cuda:0'), tensor([N2,   dim], device='cuda:0')]
    # N1 and N2 are numbers of superpixels in each gpu currently.
    N = []
    for s in all_sizes:
        N.append(s[0].item())
    # print(N, all_sizes, local_size, dim)
    max_size = max(N)

    size_diff = max_size - local_size[0].item()
    if size_diff:
        padding = torch.zeros((size_diff, dim), device="cuda", dtype=data.dtype)
        data = torch.cat((data, padding))  # make sure data in one gpu with smaller size
        # can have the same size as the data with largest size.
    all_qs_padded = [torch.zeros_like(data) for _ in range(ws)]
    torch.distributed.all_gather(all_qs_padded, data)
    all_qs = []
    for q, size in zip(all_qs_padded, N):
        all_qs.append(q[:size])
    return torch.cat(all_qs, dim=0)

def _no_grad_trunc_normal_(tensor, mean, std, a, b):
    # Cut & paste from PyTorch official master until it's in a few official releases - RW
    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf
    def norm_cdf(x):
        # Computes standard normal cumulative distribution function
        return (1. + math.erf(x / math.sqrt(2.))) / 2.

    if (mean < a - 2 * std) or (mean > b + 2 * std):
        warnings.warn("mean is more than 2 std from [a, b] in nn.init.trunc_normal_. "
                      "The distribution of values may be incorrect.",
                      stacklevel=2)

    with torch.no_grad():
        # Values are generated by using a truncated uniform distribution and
        # then using the inverse CDF for the normal distribution.
        # Get upper and lower cdf values
        l = norm_cdf((a - mean) / std)
        u = norm_cdf((b - mean) / std)

        # Uniformly fill tensor with values from [l, u], then translate to
        # [2l-1, 2u-1].
        tensor.uniform_(2 * l - 1, 2 * u - 1)

        # Use inverse cdf transform for normal distribution to get truncated
        # standard normal
        tensor.erfinv_()

        # Transform to proper mean, std
        tensor.mul_(std * math.sqrt(2.))
        tensor.add_(mean)

        # Clamp to ensure it's in the proper range
        tensor.clamp_(min=a, max=b)
        return tensor


def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):
    # type: (Tensor, float, float, float, float) -> Tensor
    return _no_grad_trunc_normal_(tensor, mean, std, a, b)


# Adopted from https://github.com/bwconrad/flexivit/blob/main/flexivit_pytorch/patch_embed.py
def pi_resize_patch_embed(
        patch_embed: Tensor,
        new_patch_size: Tuple[int, int],
        interpolation: str = "bicubic",
        antialias: bool = True,
):
    """Resample patch embedding weights to a target resolution via pseudo-inverse
    resizing.

    Based on:
        https://github.com/google-research/big_vision/blob/b00544b81f8694488d5f36295aeb7972f3755ffe/big_vision/models/proj/flexi/vit.py
        https://arxiv.org/abs/2212.08013

    Args:
        patch_embed: Patch embedding parameters of size [d, c, h, w]
        new_patch_size: Target [height, width] of embedding
        interpolation: Resize interpolation type
        antialias: Whether to apply antialiasing resizing
    Returns:
        Resized pos_embed of size [d, c h', w']
    """
    assert len(patch_embed.shape) == 4, "Patch embed kernel should be a 4D tensor"
    assert len(new_patch_size) == 2, "New patch size should only be (height, width)"

    old_patch_size = tuple(patch_embed.shape[2:])

    # Return original kernel if no resize is necessary
    if old_patch_size == new_patch_size:
        return patch_embed

    def resize(x: Tensor, shape: Tuple[int, int]):
        x_resized = F.interpolate(
            x[None, None, ...],
            shape,
            mode=interpolation,
            antialias=antialias,
        )
        return x_resized[0, 0, ...]

    def calculate_pinv(old_shape: Tuple[int, int], new_shape: Tuple[int, int]):
        mat = []
        for i in range(np.prod(old_shape)):
            basis_vec = torch.zeros(old_shape)
            basis_vec[np.unravel_index(i, old_shape)] = 1.0
            mat.append(resize(basis_vec, new_shape).reshape(-1))
        resize_matrix = torch.stack(mat)
        return torch.linalg.pinv(resize_matrix.to("cuda:0"))  # do calculation on gpu is faster

    # Calculate pseudo-inverse of resize matrix
    resize_matrix_pinv = calculate_pinv(old_patch_size, new_patch_size)
    resize_matrix_pinv = resize_matrix_pinv.to(patch_embed.device)
    print("--------", new_patch_size)

    def resample_patch_embed(patch_embed: Tensor):
        h, w = new_patch_size
        resampled_kernel = resize_matrix_pinv @ patch_embed.reshape(-1)
        return rearrange(resampled_kernel, "(h w) -> h w", h=h, w=w)

    v_resample_patch_embed = torch.vmap(torch.vmap(resample_patch_embed, 0, 0), 1, 1)
    return v_resample_patch_embed(patch_embed)

def get_random_num(num_of_st, batch_size, sample_percent, scope = "all", foreground_map = None):
    """
    For the random sampling of spatial tokens on each feature map.
    Inputs:
        num_of_st (int): nuber of spatial tokens that each feature map has. 
        batch_size (int): batch size.
        sample_percent (float): percentage of spatial tokens to be sampled on each feature map.
    Outputs:
        random_idx_list (list): list of lists, each list in this list contains the randomly sampled indice for each feature map. 
    """
    if scope == "all":
        num_selected = int(num_of_st*sample_percent)
        random_idx_list=[]
        for i in range(batch_size):
            randomlist = random.sample(range(0, num_of_st), num_selected)
            random_idx_list.append(randomlist)
    elif scope == "foreground":
        random_idx_list=[]
        for i in range(batch_size):
            foreground_idx = torch.argwhere(foreground_map[i] == 1).squeeze().tolist()
            num_selected = int(len(foreground_idx)*sample_percent)
            randomlist = random.sample(foreground_idx, num_selected)
            random_idx_list.append(randomlist)
    elif scope == "background":
        random_idx_list = []
        for i in range(batch_size):
            background_idx = torch.argwhere(foreground_map[i] == 0).squeeze().tolist()
            num_selected = int(len(background_idx) * sample_percent)
            randomlist = random.sample(background_idx, num_selected)
            random_idx_list.append(randomlist)
    return random_idx_list

def get_faiss_idx(world_size, feat_dim):
    """
    Return faiss index instance for kmeans on each gpu.
    Inputs:
        world_size (int): number of GPUs
    Outputs:
        idx (GpuIndexFlatL2): Faiss index instance.
    """
    for i in range(world_size):
        if dist.get_rank() == i:
            res = faiss.StandardGpuResources()
            cfg = faiss.GpuIndexFlatConfig()
            cfg.useFloat16 = False 
            cfg.device     = i #NOTE: Single GPU only. 
            idx = faiss.GpuIndexFlatL2(res, feat_dim, cfg)
        dist.barrier()# wait until all the gpu has cereated faiss idx. 
    return idx

def perform_faiss_kmeans(embedding_queue, k, seed, idx, niter = 20, verbose = True):
    """
    Perform kmeans
    Inputs:
        embedding_queue (tensor): features to be clustered, N x feat_dim.
        k (int): target k clusters for kmeans.
        seed (int): seed for kmeans.
        idx (GpuIndexFlatL2): Faiss index instance.
        niter (int): number of iterations of kmeans.
        verbose (bool): show verbose or not.
    Outputs:
        kmeans (Clustering): Faiss kmeans instance.
    """
    x = embedding_queue.cpu().numpy().astype('float32')
    feat_dim=x.shape[1]
    kmeans = faiss.Clustering(feat_dim,  k)
    kmeans.seed  = seed#np.random.randint(100)
    kmeans.niter = niter
    kmeans.max_points_per_centroid = 10000000
    kmeans.min_points_per_centroid = 100
    kmeans.verbose = verbose
    kmeans.train(x, idx)
    return kmeans

def concentration_estimation(faiss_idx, embeddings):
    distances, assignments = faiss_idx.search(embeddings.cpu().numpy().astype('float32'), 1)
    distances = distances.squeeze()
    print("assignments, distances++++++++++++++++", distances.shape)
    total_distances = np.sum(distances)
    num_of_embeddings = distances.shape[0]
    return total_distances/(num_of_embeddings*np.log(num_of_embeddings))

def broadcast_kmeans_seed(seed_range, gpu):
    """
    Broadcast seed for each gpu to perform kmeans using the same seed. 
    Inputs:
        seed_range (int): range of seed.
        gpu: gpu id
    Outputs:
        seed (int): seed.
    """
    if dist.get_rank() == 0:
        seed = torch.tensor(np.random.randint(seed_range)).cuda(gpu, non_blocking=True)
        dist.broadcast(seed, src=0)# broadcast seed from rank 0 to all other ranks
    else:
        seed = torch.tensor(0).cuda(gpu, non_blocking=True)
        dist.broadcast(seed, src=0)# receive seed from rank 0
    dist.barrier()# wait until all ranks have received the seed
    return int(seed.item())

def broadcast_random_idx(max_idx, percentage, gpu):
    """
    Broadcast list of random number for random updating of embedding queue.
    Inputs:
        max_idx (int): range of seed.
        percentage: gpu id
    Outputs:
        seed (int): seed.
    """
    if dist.get_rank() == 0:
        random_idx_queue = torch.tensor(random.sample(range(0, max_idx), int(max_idx*percentage))).cuda(gpu, non_blocking=True)
        dist.broadcast(random_idx_queue, src=0)# broadcast seed from rank 0 to all other ranks
    else:
        random_idx_queue = torch.tensor(random.sample(range(0, max_idx), int(max_idx*percentage))).cuda(gpu, non_blocking=True)
        dist.broadcast(random_idx_queue, src=0)# receive seed from rank 0
    dist.barrier()# wait until all ranks have received the seed
    return random_idx_queue.tolist()

def cosine_scheduler(base_value: float, final_value: float, epochs: int, niter_per_ep: int):
    # Construct cosine schedule starting at base_value and ending at final_value with epochs * niter_per_ep values.
    iters = np.arange(epochs * niter_per_ep)
    schedule = final_value + 0.5 * (base_value - final_value) * (1 + np.cos(np.pi * iters / len(iters)))
    assert len(schedule) == epochs * niter_per_ep
    return schedule

def get_params_groups(model):
    regularized = []
    not_regularized = []
    for name, param in model.named_parameters():
        if not param.requires_grad:
            continue
        # we do not regularize biases nor Norm parameters
        if name.endswith(".bias") or len(param.shape) == 1:
            not_regularized.append(param)
        else:
            regularized.append(param)
    return [{'params': regularized}, {'params': not_regularized, 'weight_decay': 0.}]

def get_binary_attn_maps(attns, threshold=0.6, blur_sigma=0.6, return_normlized_attn=False):
    # attns: N x 6 x 197 x 197
    bs = attns.shape[0]
    nh = attns.shape[1]
    cls_attns = attns[:, :, 0, 1:].reshape(bs, nh, -1)  # N x 6 x 196
    spatial_res = int(math.sqrt(cls_attns.shape[-1]))
    attn_smooth = sum(cls_attns[:, i] * 1 / nh for i in range(attns.size(1)))  # average 6 heads, N x 196
    attn_smooth = attn_smooth.reshape(bs, 1, spatial_res, spatial_res)  # N x 1 x 14 x 14
    normalized_attn = attn_smooth.reshape(bs, -1) # N x 196
    normalized_attn = F.normalize(normalized_attn, dim=1, p=2).reshape(bs, 1, spatial_res, spatial_res)
    attentions = GaussianBlur(7, sigma=(blur_sigma))(attn_smooth)
    attentions = attentions.reshape(attentions.size(0), 1, spatial_res ** 2)
    # Keep threshold% of mass
    val, idx = torch.sort(attentions)
    val /= torch.sum(val, dim=-1, keepdim=True)
    cumval = torch.cumsum(val, dim=-1)
    th_attn = cumval > (1 - threshold)
    idx2 = torch.argsort(idx)
    th_attn[:, 0] = torch.gather(th_attn[:, 0], dim=1, index=idx2[:, 0])
    th_attn = th_attn.reshape(attentions.size(0), 1, spatial_res, spatial_res).float()
    # Remove components with less than 3 pixels
    for j, th_att in enumerate(th_attn):
        labelled = label(th_att.cpu().numpy())
        for k in range(1, np.max(labelled) + 1):
            mask = labelled == k
            if np.sum(mask) <= 2:
                th_attn[j, 0][mask] = 0#mask[0]
    if return_normlized_attn:
        return th_attn.detach(), normalized_attn.detach()
    return th_attn.detach()  # N x 1 x 14 x 14


def adjust_learning_rate(optimizer, epoch, args):
    """Decay the learning rate based on schedule"""
    lr = args.lr
    if args.cos:  # cosine lr schedule
        lr *= 0.5 * (1.0 + math.cos(math.pi * epoch / args.epochs))
    else:  # stepwise lr schedule
        for milestone in args.schedule:
            lr *= 0.1 if epoch >= milestone else 1.0
    for param_group in optimizer.param_groups:
        param_group["lr"] = lr
    print("lr has been adjusted to ", lr)